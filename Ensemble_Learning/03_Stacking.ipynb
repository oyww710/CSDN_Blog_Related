{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 堆叠\n\n* 回归问题\n* 分类问题\n* 写stacking的模块"},{"metadata":{},"cell_type":"markdown","source":"### 回归问题"},{"metadata":{},"cell_type":"markdown","source":"**加载模块**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_diabetes\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.model_selection import KFold\nfrom sklearn import metrics\nimport numpy as np","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**生成训练测试集**"},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes = load_diabetes()\n\ntrain_x, train_y = diabetes.data[:400], diabetes.target[:400]\ntest_x, test_y = diabetes.data[400:], diabetes.target[400:]","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**创建基础学习器(base-learner)和元学习器（meta-learner）**"},{"metadata":{"trusted":true},"cell_type":"code","source":"## 基础学习器\n\nbase_learners = []\nknn = KNeighborsRegressor(n_neighbors=5)\n\nbase_learners.append(knn)\ndtr = DecisionTreeRegressor(max_depth=4 , random_state=123456)\n\nbase_learners.append(dtr)\nridge = Ridge()\n\nbase_learners.append(ridge)\n\n## 元学习器\nmeta_learner = LinearRegression()\n","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"初始化学习者之后，我们需要为训练集创建元数据。通过首先用KFold（n_splits = 5）指定分割数（K），然后调用KF.split（train_x），将训练集分成五个。反过来，这将返回生成训练集的五个分段的训练和测试索引。对于这些每个拆分，我们使用train_indices（四个folds）指示的数据来训练我们的基础学习器，并在与test_indices相对应的数据上创建元数据。此外，我们将每个分类器的元数据存储在meta_data数组中，并将相应的目标存储在meta_targets数组中。最后，我们转置meta_data以获得（实例，特征）形状。"},{"metadata":{},"cell_type":"markdown","source":"**在训练集创建元数据集（meta-data）**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create variables to store metadata and their targets\n\nmeta_data = np.zeros((len(base_learners), len(train_x)))\nmeta_targets = np.zeros(len(train_x))","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_data.shape","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"(3, 400)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the cross-validation folds\nKF = KFold(n_splits=5)\nmeta_index = 0\nfor train_indices, test_indices in KF.split(train_x):\n    for i in range(len(base_learners)):\n        learner = base_learners[i]\n        learner.fit(train_x[train_indices], train_y[train_indices])\n        predictions = learner.predict(train_x[test_indices])\n        meta_data[i][meta_index:meta_index+len(test_indices)] = predictions\n\n    meta_targets[meta_index:meta_index+len(test_indices)] = train_y[test_indices]\n    meta_index += len(test_indices)\n\n# Transpose the metadata to be fed into the meta-learner\nmeta_data = meta_data.transpose()","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_data","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"array([[221.        , 186.46031746, 179.44148461],\n       [ 83.2       ,  91.72477064,  94.56884758],\n       [134.4       , 186.46031746, 165.29144916],\n       ...,\n       [204.6       , 168.23076923, 160.66683682],\n       [117.4       , 168.23076923, 156.86271927],\n       [212.        , 168.23076923, 176.6069636 ]])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"对于测试集，我们不需要将其拆分为折叠。我们仅在整个训练集上训练基础学习器，并在测试集上进行预测。此外，我们评估每个基础学习者并存储评估指标，以将其与整体表现进行比较。"},{"metadata":{},"cell_type":"markdown","source":"**在测试集创建元数据集**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the metadata for the test set and evaluate the base learners\ntest_meta_data = np.zeros((len(base_learners), len(test_x)))\nbase_errors = []\nbase_r2 = []\nfor i in range(len(base_learners)):\n    learner = base_learners[i]\n    learner.fit(train_x, train_y)\n    predictions = learner.predict(test_x)\n    test_meta_data[i] = predictions\n\n    err = metrics.mean_squared_error(test_y, predictions)\n    r2 = metrics.r2_score(test_y, predictions)\n\n    base_errors.append(err)\n    base_r2.append(r2)\n\ntest_meta_data = test_meta_data.transpose()","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"现在，我们已经有了训练集和测试集的元数据集，我们可以在训练集上训练元学习器并在测试集上进行评估"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the meta-learner on the train set and evaluate it on the test set\nmeta_learner.fit(meta_data, meta_targets)\nensemble_predictions = meta_learner.predict(test_meta_data)\n\nerr = metrics.mean_squared_error(test_y, ensemble_predictions)\nr2 = metrics.r2_score(test_y, ensemble_predictions)\n\n# Print the results \nprint('ERROR R2 Name')\nprint('-'*20)\nfor i in range(len(base_learners)):\n    learner = base_learners[i]\n    print(f'{base_errors[i]:.1f} {base_r2[i]:.2f} {learner.__class__.__name__}')\nprint(f'{err:.1f} {r2:.2f} Ensemble')","execution_count":17,"outputs":[{"output_type":"stream","text":"ERROR R2 Name\n--------------------\n2697.8 0.51 KNeighborsRegressor\n3142.5 0.43 DecisionTreeRegressor\n2564.8 0.54 Ridge\n2066.6 0.63 Ensemble\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"显而易见，与最佳基础学习者相比，r平方改善了16％以上（岭回归），而MSE改善了近20％。这是一个很大的改进"},{"metadata":{},"cell_type":"markdown","source":"### 分类问题"},{"metadata":{},"cell_type":"markdown","source":"**加载模块**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_breast_cancer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn import metrics\nimport numpy as np","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**创建训练集和测试集**"},{"metadata":{"trusted":true},"cell_type":"code","source":"bc = load_breast_cancer()\n\ntrain_x, train_y = bc.data[:400], bc.target[:400]\ntest_x, test_y = bc.data[400:], bc.target[400:]","execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"下面我们初始化生成基础学习器和元学习器。其中MLPClassifier具有100个神经元的单层"},{"metadata":{},"cell_type":"markdown","source":"**初始化基础学习器和元学习器**"},{"metadata":{"trusted":true},"cell_type":"code","source":"## 基础学习器\nbase_learners = []\n\nknn = KNeighborsClassifier(n_neighbors=2)\nbase_learners.append(knn)\n\ndtr = DecisionTreeClassifier(max_depth=4, random_state=123456)\nbase_learners.append(dtr)\n\nmlpc = MLPClassifier(hidden_layer_sizes =(100, ), \n           solver='lbfgs', random_state=123456)\nbase_learners.append(mlpc)\n\n## 元学习器\nmeta_learner = LogisticRegression(solver='lbfgs')","execution_count":24,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**在训练集创建元数据集（meta-data）**"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_data = np.zeros((len(base_learners), len(train_x)))\nmeta_targets = np.zeros(len(train_x))\n\n# Create the cross-validation folds\nKF = KFold(n_splits=5)\nmeta_index = 0\nfor train_indices, test_indices in KF.split(train_x):\n    for i in range(len(base_learners)):\n        learner = base_learners[i]\n\n        learner.fit(train_x[train_indices], train_y[train_indices])\n        predictions = learner.predict_proba(train_x[test_indices])[:,0]\n\n        meta_data[i][meta_index:meta_index+len(test_indices)] = predictions\n\n    meta_targets[meta_index:meta_index+len(test_indices)] = train_y[test_indices]\n    meta_index += len(test_indices)\n\n# Transpose the metadata to be fed into the meta-learner\nmeta_data = meta_data.transpose()","execution_count":25,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**在测试集创建元数据集**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_meta_data = np.zeros((len(base_learners), len(test_x)))\nbase_acc = []\nfor i in range(len(base_learners)):\n    learner = base_learners[i]\n    learner.fit(train_x, train_y)\n    predictions = learner.predict_proba(test_x)[:,0]\n    test_meta_data[i] = predictions\n\n    acc = metrics.accuracy_score(test_y, learner.predict(test_x))\n    base_acc.append(acc)\ntest_meta_data = test_meta_data.transpose()","execution_count":26,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**训练模型打印结果**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the meta-learner on the train set and evaluate it on the test set\nmeta_learner.fit(meta_data, meta_targets)\nensemble_predictions = meta_learner.predict(test_meta_data)\n\nacc = metrics.accuracy_score(test_y, ensemble_predictions)\n\n# Print the results\nprint('Acc Name')\nprint('-'*20)\nfor i in range(len(base_learners)):\n    learner = base_learners[i]\n    print(f'{base_acc[i]:.2f} {learner.__class__.__name__}')\nprint(f'{acc:.2f} Ensemble')","execution_count":27,"outputs":[{"output_type":"stream","text":"Acc Name\n--------------------\n0.86 KNeighborsClassifier\n0.88 DecisionTreeClassifier\n0.23 MLPClassifier\n0.91 Ensemble\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### 写stacking的模块"},{"metadata":{},"cell_type":"markdown","source":"我们可以把之前的code总结一下然后写成新的模块重复使用"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import KFold\nfrom copy import deepcopy","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class StackingRegressor():\n\n    def __init__(self, learners):\n        # Create a list of sizes for each stacking level And a list of deep copied learners\n        self.level_sizes = []\n        self.learners = []\n        for learning_level in learners:\n\n            self.level_sizes.append(len(learning_level))\n            level_learners = []\n            for learner in learning_level:\n                level_learners.append(deepcopy(learner))\n            self.learners.append(level_learners)\n\n\n\n    # Creates training meta data for every level and trains each level on the previous level's meta data\n    def fit(self, x, y):\n        # Create a list of training meta data, one for each stacking level\n        # and another one for the targets. For the first level, the actual data\n        # is used.\n        meta_data = [x]\n        meta_targets = [y]\n        for i in range(len(self.learners)):\n            level_size = self.level_sizes[i]\n\n            # Create the meta data and target variables for this level\n            data_z = np.zeros((level_size, len(x)))\n            target_z = np.zeros(len(x))\n\n            train_x = meta_data[i]\n            train_y = meta_targets[i]\n\n            # Create the cross-validation folds\n            KF = KFold(n_splits=5)\n            meta_index = 0\n            for train_indices, test_indices in KF.split(x):\n                # Train each learner on the K-1 folds and create\n                # meta data for the Kth fold\n                for j in range(len(self.learners[i])):\n\n                    learner = self.learners[i][j]\n                    learner.fit(train_x[train_indices], train_y[train_indices])\n                    predictions = learner.predict(train_x[test_indices])\n\n                    data_z[j][meta_index:meta_index+len(test_indices)] = predictions\n\n                target_z[meta_index:meta_index+len(test_indices)] = train_y[test_indices]\n                meta_index += len(test_indices)\n\n            # Add the data and targets to the meta data lists\n            data_z = data_z.transpose()\n            meta_data.append(data_z)\n            meta_targets.append(target_z)\n\n\n            # Train the learner on the whole previous meta data\n            for learner in self.learners[i]:\n                    learner.fit(train_x, train_y)\n\n\n\n\n\n\n    # The predict function. Creates meta data for the test data and returns\n    # all of them. The actual predictions can be accessed with meta_data[-1]\n    def predict(self, x):\n\n        # Create a list of training meta data, one for each stacking level\n        meta_data = [x]\n        for i in range(len(self.learners)):\n            level_size = self.level_sizes[i]\n\n            data_z = np.zeros((level_size, len(x)))\n\n            test_x = meta_data[i]\n\n            # Create the cross-validation folds\n            KF = KFold(n_splits=5)\n            for train_indices, test_indices in KF.split(x):\n                # Train each learner on the K-1 folds and create\n                # meta data for the Kth fold\n                for j in range(len(self.learners[i])):\n\n                    learner = self.learners[i][j]\n                    predictions = learner.predict(test_x)\n                    data_z[j] = predictions\n\n\n\n            # Add the data and targets to the meta data lists\n            data_z = data_z.transpose()\n            meta_data.append(data_z)\n\n        # Return the meta_data the final layer's prediction can be accessed\n        # With meta_data[-1]\n        return meta_data","execution_count":32,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"然后我们可以来直接调用上面写的模块了"},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes = load_diabetes()\n\ntrain_x, train_y = diabetes.data[:400], diabetes.target[:400]\ntest_x, test_y = diabetes.data[400:], diabetes.target[400:]\n\nbase_learners = []\n\nknn = KNeighborsRegressor(n_neighbors=5)\nbase_learners.append(knn)\n\ndtr = DecisionTreeRegressor(max_depth=4, random_state=123456)\nbase_learners.append(dtr)\n\nridge = Ridge()\nbase_learners.append(ridge)\n\nmeta_learner = LinearRegression()\n\n# Instantiate the stacking regressor\nsc = StackingRegressor([[knn,dtr,ridge],[meta_learner]])\n\n# Fit and predict\nsc.fit(train_x, train_y)\nmeta_data = sc.predict(test_x)\n\n# Evaluate base learners and meta-learner\nbase_errors = []\nbase_r2 = []\nfor i in range(len(base_learners)):\n    learner = base_learners[i]\n    predictions = meta_data[1][:,i]\n    err = metrics.mean_squared_error(test_y, predictions)\n    r2 = metrics.r2_score(test_y, predictions)\n    base_errors.append(err)\n    base_r2.append(r2)\n\nerr = metrics.mean_squared_error(test_y, meta_data[-1])\nr2 = metrics.r2_score(test_y, meta_data[-1])\n\n# Print the results\nprint('ERROR R2 Name')\nprint('-'*20)\nfor i in range(len(base_learners)):\n    learner = base_learners[i]\n    print(f'{base_errors[i]:.1f} {base_r2[i]:.2f} {learner.__class__.__name__}')\nprint(f'{err:.1f} {r2:.2f} Ensemble')","execution_count":34,"outputs":[{"output_type":"stream","text":"ERROR R2 Name\n--------------------\n2697.8 0.51 KNeighborsRegressor\n3142.5 0.43 DecisionTreeRegressor\n2564.8 0.54 Ridge\n2066.6 0.63 Ensemble\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"和之前分步做的结果一样"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}